{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import (load_dataset,\n",
    "                      DatasetDict,\n",
    "                      concatenate_datasets,\n",
    "                    )\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_HUB = \"BounharAbdelaziz/AL-Atlas-Moroccan-Darija-Pretraining-Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the datasets to combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jasper_dataset = load_dataset(\"JasperV13/Darija_Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jasper_darija_instruct_dataset = load_dataset(\"JasperV13/Darija_instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abdeljalil_darija_topic_ds = load_dataset(\"abdeljalilELmajjodi/darija_topic_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abdeljalil_darija_s2s_ds = load_dataset(\"abdeljalilELmajjodi/darija_s2s_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abdeljalil_darija_qa_ds = load_dataset(\"abdeljalilELmajjodi/darija_qa_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "abdeljalil_darija_classification_ds = load_dataset(\"abdeljalilELmajjodi/darija_classification_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bourbouh_subtitles_dataset = load_dataset(\"bourbouh/moroccan-darija-youtube-subtitles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Darija_QA_dataset = load_dataset(\"Lyte/Darija-QA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_darija_merged_dataset = load_dataset(\"tachicart/mo_darija_merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------ #\n",
    "# ------------------------------------------------------------------------------------------------------------ #\n",
    "\n",
    "def extract_and_stack_dataset(dataset, dataset_source, dataset_source_column, use_source_from_dataset, metadata_columns):\n",
    "    \"\"\"\n",
    "    Stack multiple text columns into a single \"text\" column, keeping metadata, and add dataset_source as a separate column.\n",
    "\n",
    "    Args:\n",
    "        dataset (DatasetDict or Dataset): The dataset or dataset dict to process.\n",
    "        dataset_source (str): The source of the dataset to add as a separate column.\n",
    "        dataset_source_column (str): In case the dataset has the source, use it.\n",
    "        use_source_from_dataset (bool): Whether to use the 'source' column from the dataset.\n",
    "        metadata_columns (list or str): List of columns (or single column) to exclude (used as metadata).\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A DatasetDict with stacked text, metadata columns, and dataset_source as a separate column.\n",
    "    \"\"\"\n",
    "    if isinstance(metadata_columns, str):\n",
    "        metadata_columns = [metadata_columns]  # Handle single column as a list\n",
    "\n",
    "    def transform(sample):\n",
    "        # Check for columns that don't exist in the dataset\n",
    "        for col in metadata_columns:\n",
    "            if col not in sample:\n",
    "                raise KeyError(f\"Column '{col}' is not in the dataset. Available columns: {list(sample.keys())}\")\n",
    "\n",
    "        # Identify text columns to stack (all columns except excluded)\n",
    "        text_columns = [col for col in sample.keys() if col not in metadata_columns]\n",
    "        stacked_text = []\n",
    "        metadata_list = {col: [] for col in metadata_columns}  # Initialize metadata dict\n",
    "\n",
    "        # Stack text and metadata\n",
    "        for col in text_columns:\n",
    "            if isinstance(sample[col], list):  # Handle batch mapping with lists\n",
    "                stacked_text.extend(sample[col])\n",
    "                for meta_col in metadata_columns:\n",
    "                    metadata_list[meta_col].extend(sample[meta_col])\n",
    "            else:\n",
    "                stacked_text.append(sample[col])\n",
    "                for meta_col in metadata_columns:\n",
    "                    metadata_list[meta_col].append(sample[meta_col])\n",
    "\n",
    "        # Prepare metadata as string for each entry\n",
    "        merged_metadata = [\n",
    "            {meta_col: str(metadata_list[meta_col][i]) for meta_col in metadata_columns}\n",
    "            for i in range(len(stacked_text))\n",
    "        ]\n",
    "        \n",
    "        # Convert entire metadata to string (by converting the whole dict to a string)\n",
    "        merged_metadata_as_str = [str(metadata) for metadata in merged_metadata]\n",
    "       \n",
    "        # Determine the dataset_source_column value based on whether to use it from the dataset\n",
    "        if use_source_from_dataset and dataset_source_column in sample:\n",
    "            if isinstance(sample[dataset_source_column], list):  # Batch processing\n",
    "                dataset_source_column_value = sample[dataset_source_column]\n",
    "            else:\n",
    "                dataset_source_column_value = [sample[dataset_source_column]] * len(stacked_text)\n",
    "        else:\n",
    "            dataset_source_column_value = [dataset_source] * len(stacked_text)\n",
    "\n",
    "            \n",
    "        return {\n",
    "            \"text\": stacked_text,\n",
    "            \"dataset_source\": dataset_source_column_value,  # Separate column for dataset source\n",
    "            \"metadata\": merged_metadata_as_str,  # Entire metadata as string\n",
    "        }\n",
    "\n",
    "    # Apply transformation across splits, preserving split structure\n",
    "    if isinstance(dataset, DatasetDict):\n",
    "        new_splits = {}\n",
    "        for split in dataset.keys():\n",
    "            print(f\"Processing split: {split}...\")\n",
    "            new_splits[split] = dataset[split].map(\n",
    "                transform,\n",
    "                batched=True,  # Needed to handle stacking correctly\n",
    "                remove_columns=dataset[split].column_names\n",
    "            ).flatten_indices()\n",
    "        return DatasetDict(new_splits)\n",
    "    else:\n",
    "        return dataset.map(\n",
    "            transform,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        ).flatten_indices()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------ #\n",
    "# ------------------------------------------------------------------------------------------------------------ #\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "\n",
    "def concat_datasetdicts(d1: DatasetDict, d2: DatasetDict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Concatenates two DatasetDict objects, handling missing splits and columns.\n",
    "    If a split exists in one DatasetDict but not the other, it is included as-is.\n",
    "    If columns are missing in one dataset, they are added with None values.\n",
    "\n",
    "    Args:\n",
    "        d1 (DatasetDict): The first DatasetDict to concatenate.\n",
    "        d2 (DatasetDict): The second DatasetDict to concatenate.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A new DatasetDict with concatenated splits.\n",
    "    \"\"\"\n",
    "    # Get all unique splits from both DatasetDicts\n",
    "    all_splits = set(d1.keys()).union(d2.keys())\n",
    "\n",
    "    # Create a new DatasetDict by combining splits\n",
    "    concatenated = DatasetDict()\n",
    "    \n",
    "    for split in all_splits:\n",
    "        if split in d1 and split in d2:\n",
    "            # Both DatasetDicts have this split, so concatenate them\n",
    "            # Get the feature sets from both datasets\n",
    "            features_d1 = set(d1[split].column_names)\n",
    "            features_d2 = set(d2[split].column_names)\n",
    "\n",
    "            # Find missing columns in both datasets\n",
    "            missing_in_d1 = features_d2 - features_d1\n",
    "            missing_in_d2 = features_d1 - features_d2\n",
    "\n",
    "            # Add missing columns with None values\n",
    "            for missing_col in missing_in_d1:\n",
    "                d1[split] = d1[split].add_column(missing_col, [None] * len(d1[split]))\n",
    "            for missing_col in missing_in_d2:\n",
    "                d2[split] = d2[split].add_column(missing_col, [None] * len(d2[split]))\n",
    "\n",
    "            # Now concatenate the datasets with the same features\n",
    "            concatenated[split] = concatenate_datasets([d1[split], d2[split]])\n",
    "\n",
    "        elif split in d1:\n",
    "            # Only the first DatasetDict has this split\n",
    "            concatenated[split] = d1[split]\n",
    "        elif split in d2:\n",
    "            # Only the second DatasetDict has this split\n",
    "            concatenated[split] = d2[split]\n",
    "\n",
    "    return concatenated\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------ #\n",
    "# ------------------------------------------------------------------------------------------------------------ #\n",
    "\n",
    "def combine_all_datasets(all_datasets_dict: dict={}):\n",
    "    \"\"\" Combines all dataset from a given dictionary of structure. \"\"\"\n",
    "    \n",
    "    # Create a new DatasetDict by combining splits\n",
    "    combined = DatasetDict()\n",
    "        \n",
    "    for dataset_source, data in all_datasets_dict.items():\n",
    "        \n",
    "        if data[\"dataset_source_column_name\"] is not None:\n",
    "            use_source_from_dataset = True\n",
    "            dataset_source_column = data[\"dataset_source_column_name\"]\n",
    "        else:\n",
    "            use_source_from_dataset = False\n",
    "            dataset_source_column = None\n",
    "        \n",
    "        dataset = extract_and_stack_dataset(dataset=data[\"dataset\"], \n",
    "                                                dataset_source=dataset_source, \n",
    "                                                dataset_source_column=dataset_source_column, \n",
    "                                                use_source_from_dataset=use_source_from_dataset, \n",
    "                                                metadata_columns=data[\"metadata_columns\"]\n",
    "                )\n",
    "        combined = concat_datasetdicts(combined, dataset)\n",
    "        \n",
    "        print(f\"=\" * 25)\n",
    "        \n",
    "    return combined\n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------------------ #\n",
    "# ------------------------------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets_dict= {\n",
    "                \"JasperV13/Darija_Dataset\": {\n",
    "                    \"dataset\": jasper_dataset,\n",
    "                    \"metadata_columns\": ['source'],\n",
    "                    \"dataset_source_column_name\": 'source',\n",
    "                },\n",
    "                \"abdeljalilELmajjodi/darija_topic_ds\": {\n",
    "                    \"dataset\": abdeljalil_darija_topic_ds,\n",
    "                    \"metadata_columns\": ['topic'],\n",
    "                    \"dataset_source_column_name\": None,\n",
    "                },\n",
    "                \"abdeljalilELmajjodi/darija_qa_ds\": {\n",
    "                    \"dataset\": abdeljalil_darija_qa_ds,\n",
    "                    \"metadata_columns\": ['question_number', 'correct_answer_num'],\n",
    "                    \"dataset_source_column_name\": None,\n",
    "                },\n",
    "                \"abdeljalilELmajjodi/abdeljalil_darija_classification_ds\": {\n",
    "                    \"dataset\": abdeljalil_darija_classification_ds,\n",
    "                    \"metadata_columns\": ['index_id', 'category'],\n",
    "                    \"dataset_source_column_name\": None,\n",
    "                },\n",
    "                \"bourbouh/moroccan-darija-youtube-subtitles\": {\n",
    "                    \"dataset\": bourbouh_subtitles_dataset,\n",
    "                    \"metadata_columns\": ['video_id', 'title'],\n",
    "                    \"dataset_source_column_name\": None,\n",
    "                },\n",
    "                \"JasperV13/Darija_instruct\": {\n",
    "                    \"dataset\": jasper_darija_instruct_dataset,\n",
    "                    \"metadata_columns\": [],\n",
    "                    \"dataset_source_column_name\": None,\n",
    "                },\n",
    "                \"tachicart/mo_darija_merged\": {\n",
    "                    \"dataset\": mo_darija_merged_dataset,\n",
    "                    \"metadata_columns\": ['ar'],\n",
    "                    \"dataset_source_column_name\": None,\n",
    "                },\n",
    "                \"Lyte/Darija-QA\": {\n",
    "                    \"dataset\": Darija_QA_dataset,\n",
    "                    \"metadata_columns\": [],\n",
    "                    \"dataset_source_column_name\": None,\n",
    "                },\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train...\n",
      "=========================\n",
      "Processing split: train...\n",
      "=========================\n",
      "Processing split: train...\n",
      "=========================\n",
      "Processing split: train...\n",
      "=========================\n",
      "Processing split: train...\n",
      "=========================\n",
      "Processing split: train...\n",
      "=========================\n",
      "Processing split: train...\n",
      "Processing split: test...\n",
      "=========================\n",
      "Processing split: train...\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "combined = combine_all_datasets(all_datasets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'dataset_source', 'metadata'],\n",
       "        num_rows: 2774431\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'dataset_source', 'metadata'],\n",
       "        num_rows: 1820\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ArabicDarija_xP3x',\n",
       " 'Darija-Stories-Dataset',\n",
       " 'DarijaBridge',\n",
       " 'DarijaEnglish-xP3x',\n",
       " 'JasperV13/Darija_instruct',\n",
       " 'Lyte/Darija-QA',\n",
       " 'MArSum',\n",
       " 'ML101',\n",
       " 'MTCD',\n",
       " 'abdeljalilELmajjodi/abdeljalil_darija_classification_ds',\n",
       " 'abdeljalilELmajjodi/darija_qa_ds',\n",
       " 'abdeljalilELmajjodi/darija_topic_ds',\n",
       " 'atlasia/darija_english',\n",
       " 'bourbouh/moroccan-darija-youtube-subtitles',\n",
       " 'darija_speech_to_text',\n",
       " 'darija_youtube_subtitles',\n",
       " 'dataset_dyal_darija',\n",
       " 'goud-sum',\n",
       " 'moroccan_darija_wikipedia_dataset',\n",
       " 'tachicart/mo_darija_merged'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(combined['train']['dataset_source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the dataset to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 925/925 [00:01<00:00, 570.44ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 925/925 [00:01<00:00, 581.73ba/s]\n"
     ]
    }
   ],
   "source": [
    "combined.push_to_hub(DATASET_HUB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
